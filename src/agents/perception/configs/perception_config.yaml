transformer:
  num_layers: 6
  embed_dim: 512
  num_heads: 8
  ff_dim: 2048
  num_styles: 14
  max_position_embeddings: 5000
  positional_encoding: "sinusoidal"
  dropout_rate: 0.1

attention:
  dropout_rate: 0.1
  initializer: he

feedforward:
  activation: gelu
  dropout_rate: 0.1
  use_bias: true
  initializer: he

tokenizer:
  vocab_size: 50016
  max_length: 512
  pad_token: "[PAD]"
  unk_token: "[UNK]"
  cls_token: "[CLS]"
  sep_token: "[SEP]"
  bpe_model_path: "data/embeddings/bpe_200d_50k_model.json"
  bpe_vocab_path: "data/embeddings/bpe_200d_50k_vocab.json"

generator:
  temperature: 1.0
  top_k: 0
  top_p: 0.0
  repetition_penalty: 1.2

task_heads:
  classification:
    num_classes: 10
  regression: {}
  seq2seq:
    decoder:
      num_layers: 4
      embed_dim: 512  # Should match encoder dimension
      num_heads: 8
      ff_dim: 2048
      num_styles: 14
      max_position_embeddings: 5000
    dropout_rate: 0.1
  multimodal:
    hidden_dims: [512, 64]
    fusion_method: "concat"

audio_encoder:
  encoder_type: "transformer"  # or "mfcc"
  audio_length: 16000
  patch_size: 400
  in_channels: 1
  dynamic_patching: true
  positional_encoding: "learned"
  dropout_rate: 0.1
  mfcc:
    sample_rate: 16000
    n_mfcc: 13
    frame_length_ms: 25
    frame_step_ms: 10
    n_filters: 40
    low_freq: 0
    high_freq: 8000
  
vision_encoder:
  img_size: 224
  patch_size: 16
  in_channels: 3
  dropout_rate: 0.1
  positional_encoding: learned
  dynamic_patching: true
  encoder_type: transformer
  transformer:  # reused
    num_layers: 6
    embed_dim: 512
    num_heads: 8
    ff_dim: 2048
    num_styles: 14
    max_position_embeddings: 5000
  cnn:
    input_size: [224, 224]
    channels: 3
    filters:
      - [11, 11, 96]
      - [5, 5, 256]
      - [3, 3, 384]
