
adaptive_memory:
  episodic_capacity: 1000
  experience_staleness_days: 7
  semantic_decay_rate: 0.9
  semantic_threshold: 0.5
  min_memory_strength: 0.1
  replay_capacity: 50000
  drift_threshold: 0.4
  priority_alpha: 0.6
  retrieval_limit: 5
  enable_goals: True
  goal_dim: 16
  max_size: 5000
  goal_capacity: 1000
  enable_policy_grad: True
  uncertainty_dropout: 0.2
  goal_conditioning:
    goal_layers: 
      - {'neurons': 64, 'activation': 'relu'}
      - {'neurons': 16, 'activation': 'tanh'}

policy_manager:
  hidden_layers: [64, 32]
  activation: 'tanh'
  output_activation: 'softmax'
  explore: True

parameter_tuner:
  base_learning_rate: 0.01
  base_exploration_rate: 0.3
  base_discount_factor: 0.95
  base_temperature: 1.0
  min_learning_rate: 0.0001
  max_learning_rate: 0.1
  min_exploration: 0.01
  weight_decay_lambda: 0.0001
  exploration_decay: 0.9995
  target_update_interval: 100  # Hard update frequency
  base_tau: 0.005              # Soft update mixing factor
  strategy: 'bayesian'

rl:
  state_dim: 10
  action_dim: 2
  num_actions: 2
  batch_size: 64

meta_learning:
  skill_workers: 'SkillWorkerRegistry'
  training_epochs: 10

imitation_learning:
  learning_rate: 0.001
  grad_clip: 1.0
  rl_mix_ratio: 0.7
  entropy_threshold: 0.5
  continuous_actions: False
  demo_capacity: 10000
  dagger_capacity: 5000
  dagger_frequency: 5
  initial_query_prob: 0.8
  query_decay: 0.99

sgd_regressor:
  eta0: 0.01
  learning_rate: 'constant'
  alpha: 0.0001
  power_t: 0.25
  max_iter: 1000
  tol: 0.0001

neuron:
  activation_name: 'sigmoid'
  initialization_method: 'uniform_scaled'
  activation_alpha: 0.01

neural_layer:
  use_batch_norm: False
  bn_momentum: 0.9
  bn_epsilon: 0.00001
  is_training: False

neural_network:
  final_activation: 'softmax'
  loss_function_name: 'cross_entropy' # or 'mse'
  optimizer_name: 'adam' # New: 'sgd_momentum_adagrad', 'adam'
  initialization_method_default: 'he_normal'
  input_dim: 10
  layer_config:
    - neurons: 64
      activation: relu
      dropout: 0.2
      batch_norm: true
    - neurons: 32
      activation: relu
    - neurons: 2  # or the number of output classes for classification
      activation: linear

bayesian_dqn:
  num_uncertainty_samples: 10
  uncertainty_threshold: 0.1

actor_critic:
  shared_base: False
  shared_layers: []
  continuous_action: False
  initial_std: 0.5

batch_size: 64
final_activation_dim: -1
problem_type: 'multiclass_classification'  # or 'binary_classification', 'multiclass_classification'
initialization_method_default: 'xavier_uniform'
dropout_rate: 0.1
reward_normalization: True
reward_clip_range: [-5.0, 5.0]
reward_scale: 1.0
reward_bias: 0.0
reward_momentum: 0.99
reward_shaping:
  potential_type: 'goal_based'  # [l2_norm, goal_based, feature_based, learned]
  potential_features: [0, 2, 4]  # For feature_based
  learned_potential_layers: [32, 16]  # For learned potential
potential_scale: 0.1
potential_discount: 0.95
