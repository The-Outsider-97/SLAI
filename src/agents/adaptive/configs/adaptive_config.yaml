adaptive_memory:
  episodic_capacity: 1000
  experience_staleness_days: 7
  semantic_decay_rate: 0.9
  min_memory_strength: 0.1
  replay_capacity: 50000
  drift_threshold: 0.4
  priority_alpha: 0.6
  retrieval_limit: 5
  enable_goals: True
  goal_dim: 16
  max_size: 5000
  goal_capacity: 1000
  goal_conditioning:
    goal_layers: 
      - {'neurons': 64, 'activation': 'relu'}
      - {'neurons': 16, 'activation': 'tanh'}
  enable_policy_grad: True
  uncertainty_dropout: 0.2

policy_manager:
  hidden_layers: [64, 32]
  activation: 'tanh'
  output_activation: 'softmax'

parameter_tuner:
  base_learning_rate: 0.01
  base_exploration_rate: 0.3
  base_discount_factor: 0.95
  base_temperature: 1.0
  min_learning_rate: 1e-4
  max_learning_rate: 0.1
  min_exploration: 0.01
  exploration_decay: 0.9995
  target_update_interval: 100  # Hard update frequency
  base_tau: 0.005              # Soft update mixing factor

reward_normalization: True
reward_clip_range: [-5.0, 5.0]
reward_scale: 1.0
reward_bias: 0.0
reward_momentum: 0.99

reward_shaping:
  potential_type: 'goal_based'  # [l2_norm, goal_based, feature_based, learned]
  potential_features: [0, 2, 4]  # For feature_based
  learned_potential_layers: [32, 16]  # For learned potential
potential_scale: 0.1
potential_discount: 0.95

rl:
  state_dim: 4
  num_actions: 2
  batch_size: 64
